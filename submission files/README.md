# Taller 1: Predicci√≥n de Riesgo de Accidentes

Este repositorio contiene el desarrollo del Taller 1, enfocado en la creaci√≥n de modelos de machine learning para predecir el riesgo de accidentes de tr√°fico. El proyecto se basa en la competencia de Kaggle: [Playground Series - S5E10](https://www.kaggle.com/competitions/playground-series-s5e10/overview).

## üöÄ Resultados en la Competici√≥n

Los modelos fueron evaluados en la competici√≥n, obteniendo los siguientes resultados.

![Resultados en el Leaderboard](./leaderboard.png)

| Modelo                 | Score (RMSE) | Posici√≥n en Leaderboard |
| :--------------------- | :----------: | :---------------------: |
| **Ensamble Stacking** | **0.05556** |    **696** |
| Modelo Lineal (Ridge)  |   0.07199    |          2559           |

---

## ü§ñ Comparaci√≥n de Modelos

### Modelo Lineal (Baseline)

Se implement√≥ un modelo de **Regresi√≥n Ridge** como punto de partida. Este modelo fue entrenado √∫nicamente con los datos originales proporcionados. Aunque es r√°pido, su naturaleza lineal y la falta de informaci√≥n adicional limitaron su capacidad para capturar las interacciones complejas del problema. Esto se reflej√≥ en un score de **0.07199**, ubic√°ndose en la posici√≥n **2559**.

### Ensamble Stacking (Modelo Avanzado)

Para mejorar el rendimiento, se construy√≥ un **ensamble de stacking**, que result√≥ ser un enfoque muy superior por dos razones clave:

1.  **Ventaja Informativa (Datos Sint√©ticos)**: La diferencia m√°s significativa fue el uso de **aumentaci√≥n de datos**. Se utiliz√≥ una funci√≥n para generar datos sint√©ticos que, seg√∫n se infiere, replicaba la l√≥gica con la que se crearon los datos de la competencia. Al entrenar el ensamble con estos datos sint√©ticos adem√°s de los reales, el modelo obtuvo una "vista previa" de la estructura subyacente del problema. Esta fue una ventaja fundamental que el modelo lineal no tuvo.

2.  **Arquitectura de Ensamble Avanzada**:
    * **Combina Fortalezas**: El ensamble utiliz√≥ tres modelos base robustos (**XGBoost, LightGBM y CatBoost**), cada uno experto en identificar diferentes tipos de patrones.
    * **Meta-aprendizaje**: En lugar de simplemente promediar, el stacking a√±ade un "meta-modelo" (`Ridge`) que aprende a ponderar de forma inteligente las predicciones de los modelos base, creando una predicci√≥n final m√°s precisa y robusta.

Gracias a la combinaci√≥n de una arquitectura de modelo superior y, crucialmente, al entrenamiento con datos sint√©ticos informativos, el ensamble logr√≥ un score de **0.05556**, lo que nos posicion√≥ en el **lugar 696** del leaderboard.

---

## üõ†Ô∏è Estructura del Repositorio

* `/data`: Contiene los datasets de entrenamiento (`train.csv`) y prueba (`test.csv`).
* `punto1.ipynb` a `punto4.ipynb`: Notebooks con el desarrollo de la soluci√≥n, desde el an√°lisis exploratorio hasta el modelado final.
* `/submission files`: Contiene los archivos de env√≠o generados, incluyendo `submission_ensemble.csv` y `submission_lineal.csv`.
